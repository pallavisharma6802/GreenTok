

# C4Prompt : *Compress Context Cut Costs*

## Why We Built This

Every day, users and companies run into a frustrating problem: running out of credits for their favorite AI agents and GPTs. Whether you’re building a product, automating tasks, or just exploring ideas, the cost of using large language models adds up fast. Most of that cost comes from the number of tokens you send—every word, every sentence, every bit of context.

GreenTok was born out of a simple question: What if you could say more with less? By compressing prompts before they reach the AI, we help users stretch their credits further, save money, and keep their projects running smoothly. It’s not just about technical optimization—it’s about making AI more accessible and sustainable for everyone.

## The Story

We noticed that most prompts sent to LLMs are filled with greetings, polite language, and extra words that don’t actually help the AI understand your request. These words cost you money, and over time, they can mean the difference between finishing a project or hitting a paywall.

GreenTok automatically cleans up your prompts, focusing on what matters most. You get:
- **More value for your credits:** By reducing input tokens, you can send more requests for the same price.
- **Clarity and focus:** Your prompts become direct and efficient, without losing meaning.
- **A positive side effect:** According to a recent [Google Gemini study](https://cloud.google.com/blog/products/infrastructure/measuring-the-environmental-impact-of-ai-inference/), reducing the use of LLMs also reduces carbon footprints globally. Every token you save not only helps your wallet, but also helps the planet.

## What GreenTok Does

- **Removes unnecessary fluff:** Greetings, polite phrases, and filler words are stripped out.
- **Keeps your message clear:** Smart algorithms ensure the important information stays.
- **Validates clarity:** We check that the compressed prompt still means the same thing.
- **Tracks your impact:** See how many tokens, dollars, and grams of CO₂ you save—automatically.

## Why It Matters

- **Save money:** Shorter prompts mean fewer tokens, which means lower bills for AI services. For heavy users, this can mean thousands of dollars saved every year.
- **Never run out of credits unexpectedly:** By making every token count, you can plan and scale with confidence.
- **Help the planet (as a bonus):** Less compute means less energy and lower carbon emissions. The Gemini study shows that even small reductions in LLM usage add up to real environmental benefits.

## How It Works (In Simple Terms)

1. **Clean up the prompt:** Remove greetings, politeness, and filler words.
2. **Compress intelligently:** Use statistical methods to keep only the most important words and ideas.
3. **Check for clarity:** Make sure the new prompt still means the same thing.
4. **Show your savings:** Display how much you saved in cost, compute, and CO₂.

## Getting Started

1. Clone the repo and set up your Python environment.
2. Run the backend and open the web UI.
3. Paste your prompt, choose a model, and see the results.

## Project Structure

```
prompt_compressor/
├── main.py
├── config/fillers.json
├── core/
│   ├── rule_based.py
│   ├── smart_reduction.py
│   ├── validator.py
└── utils/
	├── co2_estimator.py
	└── pricing.py
app/
├── main.py (API)
static/
├── landing.html (Web UI)
```

## The Big Idea

GreenTok isn’t just about saving tokens—it’s about making AI more responsible, affordable, and accessible. By compressing prompts, we help users get more value from AI, spend less, and do their part for the environment. It’s a small change with a big impact.

---

For questions or feedback, reach out to the team!

## What We Did

We built a system that automatically shortens and optimizes user prompts for AI models. Instead of sending long, wordy requests, GreenTok cleans up the language and focuses on what matters most. Here’s how:

- **Removed unnecessary fluff:** We identified and stripped out greetings, polite phrases, and filler words that don’t add value to the prompt. This makes every request more direct and efficient.
- **Focused on meaning:** Our process keeps the core message intact. We use smart algorithms to make sure the important information stays, so the AI still understands what you want.
- **Validated clarity:** We check that the compressed prompt is still clear and similar in meaning to the original. If it’s not, we don’t use it.
- **Tracked impact:** For every prompt, we calculate how many tokens were saved, how much money that saves, and how much less energy and CO₂ is used. This gives users a real sense of their positive impact.

## Why It Matters

- **Save money:** Shorter prompts mean fewer tokens, which means lower bills for AI services.
- **Scale smarter:** Companies can handle more users and requests without increasing costs.
- **Help the planet:** Every token saved means less energy used and less carbon emitted. At scale, this makes a real difference.

## How It Works 

1. **Clean up the prompt:** Remove greetings, politeness, and filler words.
2. **Compress intelligently:** Use statistical methods to keep only the most important words and ideas.
3. **Check for clarity:** Make sure the new prompt still means the same thing.
4. **Show your savings:** Display how much you saved in cost, compute, and CO₂.

## Getting Started

1. Clone the repo and set up your Python environment.
2. Run the backend and open the web UI.
3. Paste your prompt, choose a model, and see the results.

## Project Structure

```
prompt_compressor/
├── main.py
├── config/fillers.json
├── core/
│   ├── rule_based.py
│   ├── smart_reduction.py
│   ├── validator.py
└── utils/
	├── co2_estimator.py
	└── pricing.py
app/
├── main.py (API)
static/
├── landing.html (Web UI)
```

## The Big Idea

GreenTok isn’t just about saving tokens—it’s about making AI more responsible and accessible. By compressing prompts, we help users get more value from AI, spend less, and do their part for the environment. It’s a small change with a big impact.

---

For questions or feedback, reach out to the team!


